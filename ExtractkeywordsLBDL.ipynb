{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract the keywords from the  text of the publications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py:2717: DtypeWarning: Columns (31,37,51) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "data_path = '/home/benjamin/Documents/memetracker/Data/csv/'\n",
    "pickle_data_path = '/home/benjamin/Documents/memetracker/Data/pickle/'\n",
    "series_name = 'LBDL'\n",
    "csvfile = data_path+series_name+'.csv'\n",
    "Dataf = pd.read_csv(csvfile,sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# chargement des stopwords français\n",
    "french_stopwords = set(stopwords.words('french'))\n",
    "# add custom stop words\n",
    "french_stopwords.update(['les','cette','http','https','fait','tout','tous','est','entre','dont','autour'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Extract a subset of the data\n",
    "filtered_data = Dataf[[\"date\",\"text\",\"title\", \"platform\",\"hashtags\"]].copy()\n",
    "# the entries of the date column are dates:\n",
    "filtered_data['date'] = pd.to_datetime(filtered_data['date'])#,infer_datetime_format=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@m_gael Better Call Saul, Spotless, Le Bureau des Légendes, Les Témoins, Battle Creek, Utopia made in Fincher, Daredevil, Preacher...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['@m_gael',\n",
       " 'Better',\n",
       " 'Call',\n",
       " 'Saul',\n",
       " 'Spotless',\n",
       " 'Bureau',\n",
       " 'Légendes',\n",
       " 'Témoins',\n",
       " 'Battle',\n",
       " 'Creek',\n",
       " 'Utopia',\n",
       " 'made',\n",
       " 'Fincher',\n",
       " 'Daredevil',\n",
       " 'Preacher',\n",
       " '...']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Twitter filter\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tknzr = TweetTokenizer()\n",
    "tokens_tw = tknzr.tokenize(filtered_data.text[1])\n",
    "print(filtered_data.text[1])\n",
    "#print(tokens_tw)\n",
    "#[word for word in tokens_tw if not 'http' in word]\n",
    "[token for token in tokens_tw if ((token.lower() not in french_stopwords) \n",
    "                                                              and (len(token)>2)\n",
    "                                                             and not 'http' in token)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def filter_text(dataframe):\n",
    "    # filtering the texts\n",
    "    texts = filtered_data.text\n",
    "    filtered_text_list=texts.copy()\n",
    "    for idx,row in dataframe.iterrows():\n",
    "        #print(ind)\n",
    "        texte = row.text\n",
    "        platform = row.platform\n",
    "        if (not pd.isnull(texte)):\n",
    "            if platform == 'Twitter':\n",
    "                tokens_tw = tknzr.tokenize(texte)\n",
    "                #texte_tw = \" \".join( tokens_tw )\n",
    "                #texte_tw = re.findall(r\"\\w+\", texte_tw)\n",
    "                #texte_tw = \" \".join( texte_tw )\n",
    "                #tokens_fr = nltk.tokenize.word_tokenize(texte_tw, language='french')\n",
    "                tokens_ns = [token for token in tokens_tw if ((token.lower() not in french_stopwords) \n",
    "                                                              and (len(token)>2)\n",
    "                                                             and ('http' not in token)\n",
    "                                                             and ('\\'' not in token))]\n",
    "                filtered_text = \" \".join( tokens_ns )\n",
    "            else:\n",
    "                # filter out the chars that are not alphabet letters\n",
    "                texte = re.findall(r\"\\w+\", texte)\n",
    "                texte = \" \".join( texte )\n",
    "                # Cut the text in tokens\n",
    "                tokens_fr = nltk.tokenize.word_tokenize(texte, language='french')\n",
    "                # filter out the french stop words\n",
    "                # and\n",
    "                # filter out the tokens smaller than 3 letters\n",
    "                tokens_ns = [token for token in tokens_fr if ((token.lower() not in french_stopwords) \n",
    "                                                              and (len(token)>2))]\n",
    "                filtered_text = \" \".join( tokens_ns )\n",
    "        else:\n",
    "            filtered_text = \" \"\n",
    "        filtered_text_list[idx] = filtered_text \n",
    "    return filtered_text_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filtered_texts = filter_text(filtered_data)\n",
    "filtered_data.loc[:,'filtered_text'] = filtered_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filtered_data.to_pickle(pickle_data_path+series_name+'_texts'+'.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2     Ils retour 2015 Retour sources Benjamin Castal...\n",
       "3     Télévision Colin Farrell Anthony Hopkins Lee D...\n",
       "4     devez être inscrit Passion Cinema créer Wishli...\n",
       "5     Source relaxnews Anthony Hopkins héros Westwor...\n",
       "6     club house Lbdl fermé janvier février ils atte...\n",
       "7     Journal showrunner Demain lundi janvier vais r...\n",
       "8     Journal showrunner Demain lundi janvier vais r...\n",
       "9     Colin Farrell Anthony Hopkins Lee Daniels Nigh...\n",
       "10           plateau uniquement manger #LBDL #HOLLYWOOD\n",
       "11    Nouveautés programmes danger come back très at...\n",
       "12    tournage Bureau Légendes ... #JESUISCHARLIE #l...\n",
       "13    Aujourd hui tournage Bureau Légendes JESUISCHA...\n",
       "14    Jean Maxime Renault vendredi janvier 2015 19h3...\n",
       "15    nouvelles séries françaises surveiller 2015 Je...\n",
       "16    @erochant #LBDL writing room projet LBDL nom c...\n",
       "17                                           Lbdl rompe\n",
       "18    semaine tournage Bureau Légendes JESUISCHARLIE...\n",
       "19    trouvais trop mignon cachou belle robe noir re...\n",
       "20                                         cancion lbdl\n",
       "21    semaine tourne tant figurant série bureau lége...\n",
       "22    @Odujaunet Étonné classement Bureau légendes t...\n",
       "23    Fin Tunnel lbdl mathildekraemer thisiswhatisee...\n",
       "24    Tournage Bureau légendes nouvelle série Canal ...\n",
       "Name: filtered_text, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_data.filtered_text[2:25]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting most frequent words with sci-kit learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# loading the data as a list to be processed by sci-kit learn programs\n",
    "clean_train_reviews = []\n",
    "for text in filtered_data.filtered_text:\n",
    "    clean_train_reviews.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating the bag of words...\n",
      "\n",
      "Nb of documents: 29408, Nb of features: 5000\n"
     ]
    }
   ],
   "source": [
    "print(\"Creating the bag of words...\\n\")\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Initialize the \"CountVectorizer\" object, which is scikit-learn's\n",
    "# bag of words tool.  \n",
    "vectorizer = CountVectorizer(analyzer = \"word\",   \\\n",
    "                             tokenizer = None,    \\\n",
    "                             preprocessor = None, \\\n",
    "                             stop_words = None,   \\\n",
    "                             max_features = 5000) \n",
    "\n",
    "# fit_transform() does two functions: First, it fits the model\n",
    "# and learns the vocabulary; second, it transforms our training data\n",
    "# into feature vectors. The input to fit_transform should be a list of \n",
    "# strings.\n",
    "train_data_features = vectorizer.fit_transform(clean_train_reviews)\n",
    "\n",
    "# Numpy arrays are easy to work with, so convert the result to an \n",
    "# array\n",
    "train_data_features = train_data_features.toarray()\n",
    "# Take a look at the words in the vocabulary\n",
    "vocab = vectorizer.get_feature_names()\n",
    "print('Nb of documents: '+str(train_data_features.shape[0])+', '+\n",
    "      'Nb of features: '+str(train_data_features.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bureau</td>\n",
       "      <td>27564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>saison</td>\n",
       "      <td>18066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>légendes</td>\n",
       "      <td>16166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>série</td>\n",
       "      <td>13810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>plus</td>\n",
       "      <td>12157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>legendes</td>\n",
       "      <td>10872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>france</td>\n",
       "      <td>10840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>canal</td>\n",
       "      <td>9018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>lbdl</td>\n",
       "      <td>8643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>the</td>\n",
       "      <td>6814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2015</td>\n",
       "      <td>6109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>séries</td>\n",
       "      <td>6106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>android</td>\n",
       "      <td>5896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>google</td>\n",
       "      <td>5714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>asus</td>\n",
       "      <td>5606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>samsung</td>\n",
       "      <td>5555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>galaxy</td>\n",
       "      <td>5405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>itunes</td>\n",
       "      <td>5200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>comme</td>\n",
       "      <td>5008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>tv</td>\n",
       "      <td>5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>toptvseason</td>\n",
       "      <td>4954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>bien</td>\n",
       "      <td>4885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>core</td>\n",
       "      <td>4633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>très</td>\n",
       "      <td>4498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>fr</td>\n",
       "      <td>4407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>xperia</td>\n",
       "      <td>4379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>nouvelle</td>\n",
       "      <td>3884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>aussi</td>\n",
       "      <td>3878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>lbdl_canal</td>\n",
       "      <td>3798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>2016</td>\n",
       "      <td>3725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>lenovo</td>\n",
       "      <td>3690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>acer</td>\n",
       "      <td>3673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>être</td>\n",
       "      <td>3671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>deux</td>\n",
       "      <td>3526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>kassovitz</td>\n",
       "      <td>3507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>faire</td>\n",
       "      <td>3437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>ils</td>\n",
       "      <td>3225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>msi</td>\n",
       "      <td>3211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>français</td>\n",
       "      <td>3145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>ipad</td>\n",
       "      <td>3135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>iphone</td>\n",
       "      <td>3104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>mathieu</td>\n",
       "      <td>3068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>sans</td>\n",
       "      <td>3039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>après</td>\n",
       "      <td>2971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>peu</td>\n",
       "      <td>2901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>ans</td>\n",
       "      <td>2877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>soir</td>\n",
       "      <td>2839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>épisodes</td>\n",
       "      <td>2823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>monde</td>\n",
       "      <td>2823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>mai</td>\n",
       "      <td>2798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>dgse</td>\n",
       "      <td>2721</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          words  count\n",
       "0        bureau  27564\n",
       "1        saison  18066\n",
       "2      légendes  16166\n",
       "3         série  13810\n",
       "4          plus  12157\n",
       "5      legendes  10872\n",
       "6        france  10840\n",
       "7         canal   9018\n",
       "8          lbdl   8643\n",
       "9           the   6814\n",
       "10         2015   6109\n",
       "11       séries   6106\n",
       "12      android   5896\n",
       "13       google   5714\n",
       "14         asus   5606\n",
       "15      samsung   5555\n",
       "16       galaxy   5405\n",
       "17       itunes   5200\n",
       "18        comme   5008\n",
       "19           tv   5000\n",
       "20  toptvseason   4954\n",
       "21         bien   4885\n",
       "22         core   4633\n",
       "23         très   4498\n",
       "24           fr   4407\n",
       "25       xperia   4379\n",
       "26     nouvelle   3884\n",
       "27        aussi   3878\n",
       "28   lbdl_canal   3798\n",
       "29         2016   3725\n",
       "30       lenovo   3690\n",
       "31         acer   3673\n",
       "32         être   3671\n",
       "33         deux   3526\n",
       "34    kassovitz   3507\n",
       "35        faire   3437\n",
       "36          ils   3225\n",
       "37          msi   3211\n",
       "38     français   3145\n",
       "39         ipad   3135\n",
       "40       iphone   3104\n",
       "41      mathieu   3068\n",
       "42         sans   3039\n",
       "43        après   2971\n",
       "44          peu   2901\n",
       "45          ans   2877\n",
       "46         soir   2839\n",
       "47     épisodes   2823\n",
       "48        monde   2823\n",
       "49          mai   2798\n",
       "50         dgse   2721"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sum up the counts of each vocabulary word\n",
    "dist = np.sum(train_data_features, axis=0)\n",
    "\n",
    "vocabdf = pd.DataFrame()\n",
    "vocabdf['words'] = vocab\n",
    "vocabdf['count'] = dist\n",
    "vocabdf = vocabdf.sort_values('count',ascending=False)\n",
    "vocabdf = vocabdf.reset_index(drop=True)\n",
    "# Saving the data\n",
    "vocabdf.to_pickle(pickle_data_path+series_name+'_vocab_bow'+'.pkl')\n",
    "vocabdf.loc[0:50,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating the TFIDF vectors...\n",
      "\n",
      "Nb of documents: 29408, Nb of features: 5000\n"
     ]
    }
   ],
   "source": [
    "print(\"Creating the TFIDF vectors...\\n\")\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Initialize the \"CountVectorizer\" object, which is scikit-learn's\n",
    "# bag of words tool.  \n",
    "vectorizer = TfidfVectorizer(analyzer = \"word\",   \\\n",
    "                             tokenizer = None,    \\\n",
    "                             preprocessor = None, \\\n",
    "                             stop_words = None,   \\\n",
    "                             max_features = 5000) \n",
    "\n",
    "# fit_transform() does two functions: First, it fits the model\n",
    "# and learns the vocabulary; second, it transforms our training data\n",
    "# into feature vectors. The input to fit_transform should be a list of \n",
    "# strings.\n",
    "train_data_features_tfidf = vectorizer.fit_transform(clean_train_reviews)\n",
    "\n",
    "# Numpy arrays are easy to work with, so convert the result to an \n",
    "# array\n",
    "train_data_features_tfidf = train_data_features_tfidf.toarray()\n",
    "vocab_tfidf = vectorizer.get_feature_names()\n",
    "print('Nb of documents: '+str(train_data_features_tfidf.shape[0])+', '+\n",
    "      'Nb of features: '+str(train_data_features_tfidf.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bureau</td>\n",
       "      <td>3376.168768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>legendes</td>\n",
       "      <td>3200.664637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>saison</td>\n",
       "      <td>2016.763146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>légendes</td>\n",
       "      <td>1963.050981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>lbdl</td>\n",
       "      <td>1743.376166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>tv</td>\n",
       "      <td>1585.070714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>toptvseason</td>\n",
       "      <td>1581.264513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>itunes</td>\n",
       "      <td>1567.395158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>france</td>\n",
       "      <td>1536.914318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>fr</td>\n",
       "      <td>1453.346793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>lbdl_canal</td>\n",
       "      <td>1008.880006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>série</td>\n",
       "      <td>957.413420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>lebureaudeslegendes</td>\n",
       "      <td>812.245082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>canal</td>\n",
       "      <td>811.445150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>canalplus</td>\n",
       "      <td>722.420343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>kassovitz1</td>\n",
       "      <td>663.834432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>plus</td>\n",
       "      <td>476.206728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>nouvelle</td>\n",
       "      <td>440.633557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>bureaudeslegendes</td>\n",
       "      <td>434.399179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>soir</td>\n",
       "      <td>421.890061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>séries</td>\n",
       "      <td>367.694221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>malotru</td>\n",
       "      <td>356.439517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>bien</td>\n",
       "      <td>350.267222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>kassovitz</td>\n",
       "      <td>339.818840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>gagner</td>\n",
       "      <td>330.986805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>dgse</td>\n",
       "      <td>327.662592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>très</td>\n",
       "      <td>321.139758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>erochant</td>\n",
       "      <td>310.107558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>épisodes</td>\n",
       "      <td>308.796130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>follow</td>\n",
       "      <td>286.779833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>regarder</td>\n",
       "      <td>272.756926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>mathieu</td>\n",
       "      <td>272.225693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>française</td>\n",
       "      <td>271.721102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>via</td>\n",
       "      <td>266.454008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>dvd</td>\n",
       "      <td>258.225584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>comme</td>\n",
       "      <td>236.211510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>seriescanalplus</td>\n",
       "      <td>232.136419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>secrets</td>\n",
       "      <td>225.180504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>20h55</td>\n",
       "      <td>222.451468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>aussi</td>\n",
       "      <td>222.301121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>lebureaudeslégendes</td>\n",
       "      <td>221.760645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>rochant</td>\n",
       "      <td>218.112566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>bon</td>\n",
       "      <td>216.001037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>tenter</td>\n",
       "      <td>209.474726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>lundi</td>\n",
       "      <td>205.456684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>voir</td>\n",
       "      <td>202.151871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>tournage</td>\n",
       "      <td>201.945468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>agent</td>\n",
       "      <td>201.073273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>merci</td>\n",
       "      <td>200.313922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>épisode</td>\n",
       "      <td>200.028764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>2015</td>\n",
       "      <td>198.353781</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  words        count\n",
       "0                bureau  3376.168768\n",
       "1              legendes  3200.664637\n",
       "2                saison  2016.763146\n",
       "3              légendes  1963.050981\n",
       "4                  lbdl  1743.376166\n",
       "5                    tv  1585.070714\n",
       "6           toptvseason  1581.264513\n",
       "7                itunes  1567.395158\n",
       "8                france  1536.914318\n",
       "9                    fr  1453.346793\n",
       "10           lbdl_canal  1008.880006\n",
       "11                série   957.413420\n",
       "12  lebureaudeslegendes   812.245082\n",
       "13                canal   811.445150\n",
       "14            canalplus   722.420343\n",
       "15           kassovitz1   663.834432\n",
       "16                 plus   476.206728\n",
       "17             nouvelle   440.633557\n",
       "18    bureaudeslegendes   434.399179\n",
       "19                 soir   421.890061\n",
       "20               séries   367.694221\n",
       "21              malotru   356.439517\n",
       "22                 bien   350.267222\n",
       "23            kassovitz   339.818840\n",
       "24               gagner   330.986805\n",
       "25                 dgse   327.662592\n",
       "26                 très   321.139758\n",
       "27             erochant   310.107558\n",
       "28             épisodes   308.796130\n",
       "29               follow   286.779833\n",
       "30             regarder   272.756926\n",
       "31              mathieu   272.225693\n",
       "32            française   271.721102\n",
       "33                  via   266.454008\n",
       "34                  dvd   258.225584\n",
       "35                comme   236.211510\n",
       "36      seriescanalplus   232.136419\n",
       "37              secrets   225.180504\n",
       "38                20h55   222.451468\n",
       "39                aussi   222.301121\n",
       "40  lebureaudeslégendes   221.760645\n",
       "41              rochant   218.112566\n",
       "42                  bon   216.001037\n",
       "43               tenter   209.474726\n",
       "44                lundi   205.456684\n",
       "45                 voir   202.151871\n",
       "46             tournage   201.945468\n",
       "47                agent   201.073273\n",
       "48                merci   200.313922\n",
       "49              épisode   200.028764\n",
       "50                 2015   198.353781"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sum up the counts of each vocabulary word\n",
    "dist_tfidf = np.sum(train_data_features_tfidf, axis=0)\n",
    "\n",
    "vocabdf = pd.DataFrame()\n",
    "vocabdf['words'] = vocab_tfidf\n",
    "vocabdf['count'] = dist_tfidf\n",
    "vocabdf = vocabdf.sort_values('count',ascending=False)\n",
    "vocabdf = vocabdf.reset_index(drop=True)\n",
    "# Saving the data\n",
    "vocabdf.to_pickle(pickle_data_path+series_name+'_vocab_tfidf'+'.pkl')\n",
    "vocabdf.loc[0:50,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ai',\n",
       " 'aie',\n",
       " 'aient',\n",
       " 'aies',\n",
       " 'ait',\n",
       " 'as',\n",
       " 'au',\n",
       " 'aura',\n",
       " 'aurai',\n",
       " 'auraient',\n",
       " 'aurais',\n",
       " 'aurait',\n",
       " 'auras',\n",
       " 'aurez',\n",
       " 'auriez',\n",
       " 'aurions',\n",
       " 'aurons',\n",
       " 'auront',\n",
       " 'aux',\n",
       " 'avaient',\n",
       " 'avais',\n",
       " 'avait',\n",
       " 'avec',\n",
       " 'avez',\n",
       " 'aviez',\n",
       " 'avions',\n",
       " 'avons',\n",
       " 'ayant',\n",
       " 'ayante',\n",
       " 'ayantes',\n",
       " 'ayants',\n",
       " 'ayez',\n",
       " 'ayons',\n",
       " 'c',\n",
       " 'ce',\n",
       " 'ces',\n",
       " 'cette',\n",
       " 'd',\n",
       " 'dans',\n",
       " 'de',\n",
       " 'des',\n",
       " 'du',\n",
       " 'elle',\n",
       " 'en',\n",
       " 'es',\n",
       " 'est',\n",
       " 'et',\n",
       " 'eu',\n",
       " 'eue',\n",
       " 'eues',\n",
       " 'eurent',\n",
       " 'eus',\n",
       " 'eusse',\n",
       " 'eussent',\n",
       " 'eusses',\n",
       " 'eussiez',\n",
       " 'eussions',\n",
       " 'eut',\n",
       " 'eux',\n",
       " 'eûmes',\n",
       " 'eût',\n",
       " 'eûtes',\n",
       " 'furent',\n",
       " 'fus',\n",
       " 'fusse',\n",
       " 'fussent',\n",
       " 'fusses',\n",
       " 'fussiez',\n",
       " 'fussions',\n",
       " 'fut',\n",
       " 'fûmes',\n",
       " 'fût',\n",
       " 'fûtes',\n",
       " 'http',\n",
       " 'https',\n",
       " 'il',\n",
       " 'j',\n",
       " 'je',\n",
       " 'l',\n",
       " 'la',\n",
       " 'le',\n",
       " 'les',\n",
       " 'leur',\n",
       " 'lui',\n",
       " 'm',\n",
       " 'ma',\n",
       " 'mais',\n",
       " 'me',\n",
       " 'mes',\n",
       " 'moi',\n",
       " 'mon',\n",
       " 'même',\n",
       " 'n',\n",
       " 'ne',\n",
       " 'nos',\n",
       " 'notre',\n",
       " 'nous',\n",
       " 'on',\n",
       " 'ont',\n",
       " 'ou',\n",
       " 'par',\n",
       " 'pas',\n",
       " 'pour',\n",
       " 'qu',\n",
       " 'que',\n",
       " 'qui',\n",
       " 's',\n",
       " 'sa',\n",
       " 'se',\n",
       " 'sera',\n",
       " 'serai',\n",
       " 'seraient',\n",
       " 'serais',\n",
       " 'serait',\n",
       " 'seras',\n",
       " 'serez',\n",
       " 'seriez',\n",
       " 'serions',\n",
       " 'serons',\n",
       " 'seront',\n",
       " 'ses',\n",
       " 'soient',\n",
       " 'sois',\n",
       " 'soit',\n",
       " 'sommes',\n",
       " 'son',\n",
       " 'sont',\n",
       " 'soyez',\n",
       " 'soyons',\n",
       " 'suis',\n",
       " 'sur',\n",
       " 't',\n",
       " 'ta',\n",
       " 'te',\n",
       " 'tes',\n",
       " 'toi',\n",
       " 'ton',\n",
       " 'tu',\n",
       " 'un',\n",
       " 'une',\n",
       " 'vos',\n",
       " 'votre',\n",
       " 'vous',\n",
       " 'y',\n",
       " 'à',\n",
       " 'étaient',\n",
       " 'étais',\n",
       " 'était',\n",
       " 'étant',\n",
       " 'étante',\n",
       " 'étantes',\n",
       " 'étants',\n",
       " 'étiez',\n",
       " 'étions',\n",
       " 'été',\n",
       " 'étée',\n",
       " 'étées',\n",
       " 'étés',\n",
       " 'êtes'}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "french_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
