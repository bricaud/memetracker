{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract the keywords from the  text of the publications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py:2717: DtypeWarning: Columns (37) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "data_path = '/home/benjamin/Documents/memetracker/Data/csv/'\n",
    "pickle_data_path = '/home/benjamin/Documents/memetracker/Data/pickle/'\n",
    "series_name = 'baron_noir'\n",
    "csvfile = data_path+series_name+'.csv'\n",
    "Dataf = pd.read_csv(csvfile,sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# chargement des stopwords français\n",
    "french_stopwords = set(stopwords.words('french'))\n",
    "# add custom stop words\n",
    "french_stopwords.update(['les','cette','http','https'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "french_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Extract a subset of the data\n",
    "filtered_data = Dataf[[\"date\",\"text\",\"title\", \"platform\",\"hashtags\"]].copy()\n",
    "# the entries of the date column are dates:\n",
    "filtered_data['date'] = pd.to_datetime(filtered_data['date'])#,infer_datetime_format=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Le Baron Noir, plus grande BD politique de tous les temps : http://t.co/P8JfVQ66HB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Baron', 'Noir', 'plus', 'grande', 'politique', 'tous', 'temps']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Twitter filter\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tknzr = TweetTokenizer()\n",
    "tokens_tw = tknzr.tokenize(filtered_data.text[1])\n",
    "print(filtered_data.text[1])\n",
    "#print(tokens_tw)\n",
    "#[word for word in tokens_tw if not 'http' in word]\n",
    "[token for token in tokens_tw if ((token.lower() not in french_stopwords) \n",
    "                                                              and (len(token)>2)\n",
    "                                                             and not 'http' in token)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def filter_text(dataframe):\n",
    "    # filtering the texts\n",
    "    texts = filtered_data.text\n",
    "    filtered_text_list=texts.copy()\n",
    "    for idx,row in dataframe.iterrows():\n",
    "        #print(ind)\n",
    "        texte = row.text\n",
    "        platform = row.platform\n",
    "        if (not pd.isnull(texte)):\n",
    "            if platform == 'Twitter':\n",
    "                tokens_tw = tknzr.tokenize(texte)\n",
    "                #texte_tw = \" \".join( tokens_tw )\n",
    "                #texte_tw = re.findall(r\"\\w+\", texte_tw)\n",
    "                #texte_tw = \" \".join( texte_tw )\n",
    "                #tokens_fr = nltk.tokenize.word_tokenize(texte_tw, language='french')\n",
    "                tokens_ns = [token for token in tokens_tw if ((token.lower() not in french_stopwords) \n",
    "                                                              and (len(token)>2)\n",
    "                                                             and ('http' not in token)\n",
    "                                                             and ('\\'' not in token))]\n",
    "                filtered_text = \" \".join( tokens_ns )\n",
    "            else:\n",
    "                # filter out the chars that are not alphabet letters\n",
    "                texte = re.findall(r\"\\w+\", texte)\n",
    "                texte = \" \".join( texte )\n",
    "                # Cut the text in tokens\n",
    "                tokens_fr = nltk.tokenize.word_tokenize(texte, language='french')\n",
    "                # filter out the french stop words\n",
    "                # and\n",
    "                # filter out the tokens smaller than 3 letters\n",
    "                tokens_ns = [token for token in tokens_fr if ((token.lower() not in french_stopwords) \n",
    "                                                              and (len(token)>2))]\n",
    "                filtered_text = \" \".join( tokens_ns )\n",
    "        else:\n",
    "            filtered_text = \" \"\n",
    "        filtered_text_list[idx] = filtered_text \n",
    "    return filtered_text_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filtered_texts = filter_text(filtered_data)\n",
    "filtered_data.loc[:,'filtered_text'] = filtered_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filtered_data.to_pickle(pickle_data_path+series_name+'_texts'+'.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2                                      Baron Noir suite\n",
       "3     Excellent teeeellement vrai @OLecointe Baron N...\n",
       "4                       @Pward13 Baron Noir plus classe\n",
       "5     Situé route Sharm Sheikh Dahab saura charmer a...\n",
       "6     Peux dire plus étrier Montage réglages peux pa...\n",
       "7     maertens écrit Peux dire plus étrier Montage r...\n",
       "8     confirme Statistiques Posté rickyfirst Jeu Jan...\n",
       "9     maertens écrit Peux dire plus étrier Montage r...\n",
       "10    Merci gars Statistiques Posté maertens Jeu Jan...\n",
       "11    maertens écrit Peux dire plus étrier Montage r...\n",
       "12    Paroles chanson baron noir chantée Arthur baro...\n",
       "13    Billet PASCAL PORCHER CROISIERE BARON NOIR SOU...\n",
       "14    Discussion publiée PASCAL PORCHER CROISIERE SO...\n",
       "15    CROISIERE BARON NOIR SOUDAN FEVRIER 2015 URGEN...\n",
       "16    @scuba_people CROISIERE BARON NOIR SOUDAN FEVR...\n",
       "17    Bonjour tous voudrais mettre leviers bout prol...\n",
       "18         Nouveau coup coeur Arthur Baron Noir #deezer\n",
       "19    Top Ten Tuesday TTT donc rendez hebdomadaire l...\n",
       "20    Cabu mort assassiné ans attentat frappé journa...\n",
       "21    Cabu mort assassiné ans attentat frappé journa...\n",
       "22    Petit changement niveau pédalier Suite soucis ...\n",
       "23    Très joli pédalier Statistiques Posté webjo Me...\n",
       "24    Beau pédalier effectivement profité tarif avan...\n",
       "25    testé nouveau pédalier aujourd hui parcours va...\n",
       "26              Ostende Baron Noir Live Thou Bout Chant\n",
       "27                                           baron noir\n",
       "28    @_Falbala Excellent teeeellement vrai @OLecoin...\n",
       "29    @_Falbala Excellent teeeellement vrai @OLecoin...\n",
       "30    Nouvelle Version Arthur baron noir 1996 Vidéo ...\n",
       "31    pince Comment pourrait avoir pris points sonda...\n",
       "32    année décidé céder habitude assez répandue blo...\n",
       "33    @Levy_Ein baron noir assassin arrive toujours ...\n",
       "34    Croyez Grecs assez stupides jeter telle aventu...\n",
       "35     Arthur baron noir 1996 Vidéo paroles #FrenchSong\n",
       "36    mur Berlin effondré ans toujours donc fait Cub...\n",
       "37    Publié janvier 2015 Shelton Voici celles décou...\n",
       "38    député Laurent Louis relayé mail collectif ano...\n",
       "39    @imaginelf Finalement relis Baron Noir Gechter...\n",
       "40       Baron Noir avance vitesse croisière #BaronNoir\n",
       "41    @ogechter Baron Noir avance vitesse croisière ...\n",
       "42                          voulez voir plan #BaronNoir\n",
       "43                @ogechter voulez voir plan #BaronNoir\n",
       "44    Citation Abso12 écrit veux faire rabat joie fr...\n",
       "45    Verser systématiquement reliquat droit plutôt ...\n",
       "46    acc ben connu Métal Hurlant Fluide Glacial dir...\n",
       "47    hop page tournée 2014 pseudo panne lecture aus...\n",
       "48    baron noir écrit demande cas peut faire piquer...\n",
       "49    Epopée Surinienne rencontre Teubéniens Teubéni...\n",
       "Name: filtered_text, dtype: object"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_data.filtered_text[2:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting most frequent words with sci-kit learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# loading the data as a list to be processed by sci-kit learn programs\n",
    "clean_train_reviews = []\n",
    "for text in filtered_data.filtered_text:\n",
    "    clean_train_reviews.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating the bag of words...\n",
      "\n",
      "Nb of documents: 29471, Nb of features: 5000\n"
     ]
    }
   ],
   "source": [
    "print(\"Creating the bag of words...\\n\")\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Initialize the \"CountVectorizer\" object, which is scikit-learn's\n",
    "# bag of words tool.  \n",
    "vectorizer = CountVectorizer(analyzer = \"word\",   \\\n",
    "                             tokenizer = None,    \\\n",
    "                             preprocessor = None, \\\n",
    "                             stop_words = None,   \\\n",
    "                             max_features = 5000) \n",
    "\n",
    "# fit_transform() does two functions: First, it fits the model\n",
    "# and learns the vocabulary; second, it transforms our training data\n",
    "# into feature vectors. The input to fit_transform should be a list of \n",
    "# strings.\n",
    "train_data_features = vectorizer.fit_transform(clean_train_reviews)\n",
    "\n",
    "# Numpy arrays are easy to work with, so convert the result to an \n",
    "# array\n",
    "train_data_features = train_data_features.toarray()\n",
    "# Take a look at the words in the vocabulary\n",
    "vocab = vectorizer.get_feature_names()\n",
    "print('Nb of documents: '+str(train_data_features.shape[0])+', '+\n",
    "      'Nb of features: '+str(train_data_features.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>noir</td>\n",
       "      <td>19132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>baron</td>\n",
       "      <td>18939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>série</td>\n",
       "      <td>16201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>baronnoir</td>\n",
       "      <td>15575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>plus</td>\n",
       "      <td>10725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>politique</td>\n",
       "      <td>9174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>canal</td>\n",
       "      <td>9037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>casting</td>\n",
       "      <td>6580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2016</td>\n",
       "      <td>6385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>tout</td>\n",
       "      <td>5881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>france</td>\n",
       "      <td>5549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>kad</td>\n",
       "      <td>4889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>bien</td>\n",
       "      <td>4819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>comme</td>\n",
       "      <td>4745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>saison</td>\n",
       "      <td>4728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>fait</td>\n",
       "      <td>4688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>paris</td>\n",
       "      <td>4639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>entre</td>\n",
       "      <td>4542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>merad</td>\n",
       "      <td>4541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>ans</td>\n",
       "      <td>4414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>aussi</td>\n",
       "      <td>4300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>très</td>\n",
       "      <td>4092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>deux</td>\n",
       "      <td>3710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>lire</td>\n",
       "      <td>3651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>faire</td>\n",
       "      <td>3462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>être</td>\n",
       "      <td>3437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>jusqu</td>\n",
       "      <td>3347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>juin</td>\n",
       "      <td>3342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>loi</td>\n",
       "      <td>3333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>canalplus</td>\n",
       "      <td>3324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>travail</td>\n",
       "      <td>3298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>séries</td>\n",
       "      <td>3254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>nouvelle</td>\n",
       "      <td>3253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>tous</td>\n",
       "      <td>3121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>après</td>\n",
       "      <td>2976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>homme</td>\n",
       "      <td>2949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>ils</td>\n",
       "      <td>2873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>sans</td>\n",
       "      <td>2782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>contre</td>\n",
       "      <td>2761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>politiques</td>\n",
       "      <td>2724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>quand</td>\n",
       "      <td>2557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>encore</td>\n",
       "      <td>2465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>partie</td>\n",
       "      <td>2442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>2015</td>\n",
       "      <td>2414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>hollande</td>\n",
       "      <td>2360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>monde</td>\n",
       "      <td>2342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>soir</td>\n",
       "      <td>2338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>peut</td>\n",
       "      <td>2337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>article</td>\n",
       "      <td>2312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>peu</td>\n",
       "      <td>2280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>film</td>\n",
       "      <td>2278</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         words  count\n",
       "0         noir  19132\n",
       "1        baron  18939\n",
       "2        série  16201\n",
       "3    baronnoir  15575\n",
       "4         plus  10725\n",
       "5    politique   9174\n",
       "6        canal   9037\n",
       "7      casting   6580\n",
       "8         2016   6385\n",
       "9         tout   5881\n",
       "10      france   5549\n",
       "11         kad   4889\n",
       "12        bien   4819\n",
       "13       comme   4745\n",
       "14      saison   4728\n",
       "15        fait   4688\n",
       "16       paris   4639\n",
       "17       entre   4542\n",
       "18       merad   4541\n",
       "19         ans   4414\n",
       "20       aussi   4300\n",
       "21        très   4092\n",
       "22        deux   3710\n",
       "23        lire   3651\n",
       "24       faire   3462\n",
       "25        être   3437\n",
       "26       jusqu   3347\n",
       "27        juin   3342\n",
       "28         loi   3333\n",
       "29   canalplus   3324\n",
       "30     travail   3298\n",
       "31      séries   3254\n",
       "32    nouvelle   3253\n",
       "33        tous   3121\n",
       "34       après   2976\n",
       "35       homme   2949\n",
       "36         ils   2873\n",
       "37        sans   2782\n",
       "38      contre   2761\n",
       "39  politiques   2724\n",
       "40       quand   2557\n",
       "41      encore   2465\n",
       "42      partie   2442\n",
       "43        2015   2414\n",
       "44    hollande   2360\n",
       "45       monde   2342\n",
       "46        soir   2338\n",
       "47        peut   2337\n",
       "48     article   2312\n",
       "49         peu   2280\n",
       "50        film   2278"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sum up the counts of each vocabulary word\n",
    "dist = np.sum(train_data_features, axis=0)\n",
    "\n",
    "vocabdf = pd.DataFrame()\n",
    "vocabdf['words'] = vocab\n",
    "vocabdf['count'] = dist\n",
    "vocabdf = vocabdf.sort_values('count',ascending=False)\n",
    "vocabdf = vocabdf.reset_index(drop=True)\n",
    "# Saving the data\n",
    "vocabdf.to_pickle(pickle_data_path+series_name+'_vocab_bow'+'.pkl')\n",
    "vocabdf.loc[0:50,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating the TFIDF vectors...\n",
      "\n",
      "Nb of documents: 29471, Nb of features: 5000\n"
     ]
    }
   ],
   "source": [
    "print(\"Creating the TFIDF vectors...\\n\")\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Initialize the \"CountVectorizer\" object, which is scikit-learn's\n",
    "# bag of words tool.  \n",
    "vectorizer = TfidfVectorizer(analyzer = \"word\",   \\\n",
    "                             tokenizer = None,    \\\n",
    "                             preprocessor = None, \\\n",
    "                             stop_words = None,   \\\n",
    "                             max_features = 5000) \n",
    "\n",
    "# fit_transform() does two functions: First, it fits the model\n",
    "# and learns the vocabulary; second, it transforms our training data\n",
    "# into feature vectors. The input to fit_transform should be a list of \n",
    "# strings.\n",
    "train_data_features_tfidf = vectorizer.fit_transform(clean_train_reviews)\n",
    "\n",
    "# Numpy arrays are easy to work with, so convert the result to an \n",
    "# array\n",
    "train_data_features_tfidf = train_data_features_tfidf.toarray()\n",
    "vocab_tfidf = vectorizer.get_feature_names()\n",
    "print('Nb of documents: '+str(train_data_features_tfidf.shape[0])+', '+\n",
    "      'Nb of features: '+str(train_data_features_tfidf.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>baronnoir</td>\n",
       "      <td>2342.811153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>noir</td>\n",
       "      <td>2058.330993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>baron</td>\n",
       "      <td>2056.237946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>série</td>\n",
       "      <td>1358.085107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>canal</td>\n",
       "      <td>941.503291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>politique</td>\n",
       "      <td>933.103956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>canalplus</td>\n",
       "      <td>870.112593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>kad</td>\n",
       "      <td>676.008663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>merad</td>\n",
       "      <td>621.625339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>saison</td>\n",
       "      <td>610.867047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>plus</td>\n",
       "      <td>567.460783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>soir</td>\n",
       "      <td>434.254782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>tout</td>\n",
       "      <td>416.290175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>bien</td>\n",
       "      <td>404.365581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>kadmerad</td>\n",
       "      <td>389.074269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>via</td>\n",
       "      <td>338.429719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>nouvelle</td>\n",
       "      <td>333.496080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>très</td>\n",
       "      <td>319.115732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>hollande</td>\n",
       "      <td>314.533528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>fait</td>\n",
       "      <td>306.084573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>dvd</td>\n",
       "      <td>290.984472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>séries</td>\n",
       "      <td>285.414554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>regarder</td>\n",
       "      <td>282.830244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>bonne</td>\n",
       "      <td>282.533007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>quand</td>\n",
       "      <td>275.944607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>gagner</td>\n",
       "      <td>274.463698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>aussi</td>\n",
       "      <td>260.749196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>follow</td>\n",
       "      <td>260.454726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>épisodes</td>\n",
       "      <td>256.693618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>fiction</td>\n",
       "      <td>256.493962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>française</td>\n",
       "      <td>250.849045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>dit</td>\n",
       "      <td>248.555781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>comme</td>\n",
       "      <td>240.603360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>épisode</td>\n",
       "      <td>239.407025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>bon</td>\n",
       "      <td>229.059902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>politiques</td>\n",
       "      <td>227.202673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>seriescanalplus</td>\n",
       "      <td>225.750260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>voir</td>\n",
       "      <td>220.230650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>vraiment</td>\n",
       "      <td>218.011630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>house</td>\n",
       "      <td>210.253815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>vrai</td>\n",
       "      <td>205.997869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>cards</td>\n",
       "      <td>205.847441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>avant</td>\n",
       "      <td>205.255113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>demain</td>\n",
       "      <td>205.068909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>dray</td>\n",
       "      <td>204.696726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>réalité</td>\n",
       "      <td>204.518605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>tournage</td>\n",
       "      <td>204.319229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>déjà</td>\n",
       "      <td>197.454944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>merci</td>\n",
       "      <td>192.822083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>france</td>\n",
       "      <td>191.351381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>deux</td>\n",
       "      <td>189.065293</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              words        count\n",
       "0         baronnoir  2342.811153\n",
       "1              noir  2058.330993\n",
       "2             baron  2056.237946\n",
       "3             série  1358.085107\n",
       "4             canal   941.503291\n",
       "5         politique   933.103956\n",
       "6         canalplus   870.112593\n",
       "7               kad   676.008663\n",
       "8             merad   621.625339\n",
       "9            saison   610.867047\n",
       "10             plus   567.460783\n",
       "11             soir   434.254782\n",
       "12             tout   416.290175\n",
       "13             bien   404.365581\n",
       "14         kadmerad   389.074269\n",
       "15              via   338.429719\n",
       "16         nouvelle   333.496080\n",
       "17             très   319.115732\n",
       "18         hollande   314.533528\n",
       "19             fait   306.084573\n",
       "20              dvd   290.984472\n",
       "21           séries   285.414554\n",
       "22         regarder   282.830244\n",
       "23            bonne   282.533007\n",
       "24            quand   275.944607\n",
       "25           gagner   274.463698\n",
       "26            aussi   260.749196\n",
       "27           follow   260.454726\n",
       "28         épisodes   256.693618\n",
       "29          fiction   256.493962\n",
       "30        française   250.849045\n",
       "31              dit   248.555781\n",
       "32            comme   240.603360\n",
       "33          épisode   239.407025\n",
       "34              bon   229.059902\n",
       "35       politiques   227.202673\n",
       "36  seriescanalplus   225.750260\n",
       "37             voir   220.230650\n",
       "38         vraiment   218.011630\n",
       "39            house   210.253815\n",
       "40             vrai   205.997869\n",
       "41            cards   205.847441\n",
       "42            avant   205.255113\n",
       "43           demain   205.068909\n",
       "44             dray   204.696726\n",
       "45          réalité   204.518605\n",
       "46         tournage   204.319229\n",
       "47             déjà   197.454944\n",
       "48            merci   192.822083\n",
       "49           france   191.351381\n",
       "50             deux   189.065293"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sum up the counts of each vocabulary word\n",
    "dist_tfidf = np.sum(train_data_features_tfidf, axis=0)\n",
    "\n",
    "vocabdf = pd.DataFrame()\n",
    "vocabdf['words'] = vocab_tfidf\n",
    "vocabdf['count'] = dist_tfidf\n",
    "vocabdf = vocabdf.sort_values('count',ascending=False)\n",
    "vocabdf = vocabdf.reset_index(drop=True)\n",
    "# Saving the data\n",
    "vocabdf.to_pickle(pickle_data_path+series_name+'_vocab_tfidf'+'.pkl')\n",
    "vocabdf.loc[0:50,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for text in filtered_data.filtered_text:\n",
    "    if '\\'un ' in text:\n",
    "        print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ai',\n",
       " 'aie',\n",
       " 'aient',\n",
       " 'aies',\n",
       " 'ait',\n",
       " 'as',\n",
       " 'au',\n",
       " 'aura',\n",
       " 'aurai',\n",
       " 'auraient',\n",
       " 'aurais',\n",
       " 'aurait',\n",
       " 'auras',\n",
       " 'aurez',\n",
       " 'auriez',\n",
       " 'aurions',\n",
       " 'aurons',\n",
       " 'auront',\n",
       " 'aux',\n",
       " 'avaient',\n",
       " 'avais',\n",
       " 'avait',\n",
       " 'avec',\n",
       " 'avez',\n",
       " 'aviez',\n",
       " 'avions',\n",
       " 'avons',\n",
       " 'ayant',\n",
       " 'ayante',\n",
       " 'ayantes',\n",
       " 'ayants',\n",
       " 'ayez',\n",
       " 'ayons',\n",
       " 'c',\n",
       " 'ce',\n",
       " 'ces',\n",
       " 'cette',\n",
       " 'd',\n",
       " 'dans',\n",
       " 'de',\n",
       " 'des',\n",
       " 'du',\n",
       " 'elle',\n",
       " 'en',\n",
       " 'es',\n",
       " 'est',\n",
       " 'et',\n",
       " 'eu',\n",
       " 'eue',\n",
       " 'eues',\n",
       " 'eurent',\n",
       " 'eus',\n",
       " 'eusse',\n",
       " 'eussent',\n",
       " 'eusses',\n",
       " 'eussiez',\n",
       " 'eussions',\n",
       " 'eut',\n",
       " 'eux',\n",
       " 'eûmes',\n",
       " 'eût',\n",
       " 'eûtes',\n",
       " 'furent',\n",
       " 'fus',\n",
       " 'fusse',\n",
       " 'fussent',\n",
       " 'fusses',\n",
       " 'fussiez',\n",
       " 'fussions',\n",
       " 'fut',\n",
       " 'fûmes',\n",
       " 'fût',\n",
       " 'fûtes',\n",
       " 'http',\n",
       " 'https',\n",
       " 'il',\n",
       " 'j',\n",
       " 'je',\n",
       " 'l',\n",
       " 'la',\n",
       " 'le',\n",
       " 'les',\n",
       " 'leur',\n",
       " 'lui',\n",
       " 'm',\n",
       " 'ma',\n",
       " 'mais',\n",
       " 'me',\n",
       " 'mes',\n",
       " 'moi',\n",
       " 'mon',\n",
       " 'même',\n",
       " 'n',\n",
       " 'ne',\n",
       " 'nos',\n",
       " 'notre',\n",
       " 'nous',\n",
       " 'on',\n",
       " 'ont',\n",
       " 'ou',\n",
       " 'par',\n",
       " 'pas',\n",
       " 'pour',\n",
       " 'qu',\n",
       " 'que',\n",
       " 'qui',\n",
       " 's',\n",
       " 'sa',\n",
       " 'se',\n",
       " 'sera',\n",
       " 'serai',\n",
       " 'seraient',\n",
       " 'serais',\n",
       " 'serait',\n",
       " 'seras',\n",
       " 'serez',\n",
       " 'seriez',\n",
       " 'serions',\n",
       " 'serons',\n",
       " 'seront',\n",
       " 'ses',\n",
       " 'soient',\n",
       " 'sois',\n",
       " 'soit',\n",
       " 'sommes',\n",
       " 'son',\n",
       " 'sont',\n",
       " 'soyez',\n",
       " 'soyons',\n",
       " 'suis',\n",
       " 'sur',\n",
       " 't',\n",
       " 'ta',\n",
       " 'te',\n",
       " 'tes',\n",
       " 'toi',\n",
       " 'ton',\n",
       " 'tu',\n",
       " 'un',\n",
       " 'une',\n",
       " 'vos',\n",
       " 'votre',\n",
       " 'vous',\n",
       " 'y',\n",
       " 'à',\n",
       " 'étaient',\n",
       " 'étais',\n",
       " 'était',\n",
       " 'étant',\n",
       " 'étante',\n",
       " 'étantes',\n",
       " 'étants',\n",
       " 'étiez',\n",
       " 'étions',\n",
       " 'été',\n",
       " 'étée',\n",
       " 'étées',\n",
       " 'étés',\n",
       " 'êtes'}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "french_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
