{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract the keywords from the  text of the publications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py:2717: DtypeWarning: Columns (31,37) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "data_path = '/home/benjamin/Documents/memetracker/Data/csv/'\n",
    "pickle_data_path = '/home/benjamin/Documents/memetracker/Data/pickle/'\n",
    "series_name = 'marseille'\n",
    "csvfile = data_path+series_name+'.csv'\n",
    "Dataf = pd.read_csv(csvfile,sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# chargement des stopwords français\n",
    "french_stopwords = set(stopwords.words('french'))\n",
    "# add custom stop words\n",
    "french_stopwords.update(['les','cette','http','https','fait','tout','tous','est'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Extract a subset of the data\n",
    "filtered_data = Dataf[[\"date\",\"text\",\"title\", \"platform\",\"hashtags\"]].copy()\n",
    "# the entries of the date column are dates:\n",
    "filtered_data['date'] = pd.to_datetime(filtered_data['date'])#,infer_datetime_format=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Marseille – nova série da Netflix http://t.co/lLG7hdg3DB #SdTV\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Marseille', 'nova', 'série', 'Netflix', '#SdTV']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Twitter filter\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tknzr = TweetTokenizer()\n",
    "tokens_tw = tknzr.tokenize(filtered_data.text[1])\n",
    "print(filtered_data.text[1])\n",
    "#print(tokens_tw)\n",
    "#[word for word in tokens_tw if not 'http' in word]\n",
    "[token for token in tokens_tw if ((token.lower() not in french_stopwords) \n",
    "                                                              and (len(token)>2)\n",
    "                                                             and not 'http' in token)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def filter_text(dataframe):\n",
    "    # filtering the texts\n",
    "    texts = filtered_data.text\n",
    "    filtered_text_list=texts.copy()\n",
    "    for idx,row in dataframe.iterrows():\n",
    "        #print(ind)\n",
    "        texte = row.text\n",
    "        platform = row.platform\n",
    "        if (not pd.isnull(texte)):\n",
    "            if platform == 'Twitter':\n",
    "                tokens_tw = tknzr.tokenize(texte)\n",
    "                #texte_tw = \" \".join( tokens_tw )\n",
    "                #texte_tw = re.findall(r\"\\w+\", texte_tw)\n",
    "                #texte_tw = \" \".join( texte_tw )\n",
    "                #tokens_fr = nltk.tokenize.word_tokenize(texte_tw, language='french')\n",
    "                tokens_ns = [token for token in tokens_tw if ((token.lower() not in french_stopwords) \n",
    "                                                              and (len(token)>2)\n",
    "                                                             and ('http' not in token)\n",
    "                                                             and ('\\'' not in token))]\n",
    "                filtered_text = \" \".join( tokens_ns )\n",
    "            else:\n",
    "                # filter out the chars that are not alphabet letters\n",
    "                texte = re.findall(r\"\\w+\", texte)\n",
    "                texte = \" \".join( texte )\n",
    "                # Cut the text in tokens\n",
    "                tokens_fr = nltk.tokenize.word_tokenize(texte, language='french')\n",
    "                # filter out the french stop words\n",
    "                # and\n",
    "                # filter out the tokens smaller than 3 letters\n",
    "                tokens_ns = [token for token in tokens_fr if ((token.lower() not in french_stopwords) \n",
    "                                                              and (len(token)>2))]\n",
    "                filtered_text = \" \".join( tokens_ns )\n",
    "        else:\n",
    "            filtered_text = \" \"\n",
    "        filtered_text_list[idx] = filtered_text \n",
    "    return filtered_text_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filtered_texts = filter_text(filtered_data)\n",
    "filtered_data.loc[:,'filtered_text'] = filtered_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filtered_data.to_pickle(pickle_data_path+series_name+'_texts'+'.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2     @ChristineBeaume @jl_burger fait penser serie ...\n",
       "3     Originally Posted Durack vraiment bien aimé sa...\n",
       "4     Originally Posted Sharn émerge oui bon hein sé...\n",
       "5     émerge oui bon hein série Marseille 2015 Netfl...\n",
       "6     Série #Netflix Marseille annonce casting févri...\n",
       "7     015 marquera nouvelle ère Plusieurs historiens...\n",
       "8     Sollicités Provence ils évoquent politique soc...\n",
       "9     Ils mettent table Paul Wermus TILLINAC Manuel ...\n",
       "10    janvier 2015 rigole bien mythique prison marse...\n",
       "11    pourquoi pas5 janvier 2015 rejoins complètemen...\n",
       "12    Vite dit grrr rigole bien mythique prison mars...\n",
       "13    pourquoi pas5 janvier 2015 rejoins complètemen...\n",
       "14    janvier 2015 lagazettedeputeaux Vincent Coquaz...\n",
       "15    dentiste retrouvé égorgé cabinet Marseille Réd...\n",
       "16    Prisons silence univers carcéral semble faire ...\n",
       "17    financement films français jamais aussi diffic...\n",
       "18    Boohoo avis Après Marseille embarque série art...\n",
       "19    regarde série policière Marseille sais plus co...\n",
       "20    #CharlieHebdo Fausses alertes établissements s...\n",
       "21    #CharlieHebdo Fausses alertes établissements s...\n",
       "22    @Zack_Nani lyon dit aprés marseille temps mieu...\n",
       "23    dim janv 2015 armes Kalachnikov banlieues fusi...\n",
       "24    fusillade Lille met nouvelle fois lumière bana...\n",
       "Name: filtered_text, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_data.filtered_text[2:25]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting most frequent words with sci-kit learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# loading the data as a list to be processed by sci-kit learn programs\n",
    "clean_train_reviews = []\n",
    "for text in filtered_data.filtered_text:\n",
    "    clean_train_reviews.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating the bag of words...\n",
      "\n",
      "Nb of documents: 32394, Nb of features: 5000\n"
     ]
    }
   ],
   "source": [
    "print(\"Creating the bag of words...\\n\")\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Initialize the \"CountVectorizer\" object, which is scikit-learn's\n",
    "# bag of words tool.  \n",
    "vectorizer = CountVectorizer(analyzer = \"word\",   \\\n",
    "                             tokenizer = None,    \\\n",
    "                             preprocessor = None, \\\n",
    "                             stop_words = None,   \\\n",
    "                             max_features = 5000) \n",
    "\n",
    "# fit_transform() does two functions: First, it fits the model\n",
    "# and learns the vocabulary; second, it transforms our training data\n",
    "# into feature vectors. The input to fit_transform should be a list of \n",
    "# strings.\n",
    "train_data_features = vectorizer.fit_transform(clean_train_reviews)\n",
    "\n",
    "# Numpy arrays are easy to work with, so convert the result to an \n",
    "# array\n",
    "train_data_features = train_data_features.toarray()\n",
    "# Take a look at the words in the vocabulary\n",
    "vocab = vectorizer.get_feature_names()\n",
    "print('Nb of documents: '+str(train_data_features.shape[0])+', '+\n",
    "      'Nb of features: '+str(train_data_features.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>marseille</td>\n",
       "      <td>39257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>série</td>\n",
       "      <td>24919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>netflix</td>\n",
       "      <td>20464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>plus</td>\n",
       "      <td>12718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2016</td>\n",
       "      <td>10807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>marseillenetflix</td>\n",
       "      <td>10587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>depardieu</td>\n",
       "      <td>8659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>france</td>\n",
       "      <td>8531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>bien</td>\n",
       "      <td>7588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>mai</td>\n",
       "      <td>7462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>gérard</td>\n",
       "      <td>6596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>fait</td>\n",
       "      <td>6457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>tout</td>\n",
       "      <td>5846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>première</td>\n",
       "      <td>5443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>comme</td>\n",
       "      <td>5439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>ans</td>\n",
       "      <td>5243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>deux</td>\n",
       "      <td>5018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>après</td>\n",
       "      <td>4933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>ville</td>\n",
       "      <td>4764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>serie</td>\n",
       "      <td>4670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>saison</td>\n",
       "      <td>4582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>être</td>\n",
       "      <td>4461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>française</td>\n",
       "      <td>4446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>marseilleserie</td>\n",
       "      <td>4446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>magimel</td>\n",
       "      <td>4315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>très</td>\n",
       "      <td>4115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>maire</td>\n",
       "      <td>3967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>tf1</td>\n",
       "      <td>3924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>faire</td>\n",
       "      <td>3882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>aussi</td>\n",
       "      <td>3818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>seul</td>\n",
       "      <td>3782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>sans</td>\n",
       "      <td>3624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>épisodes</td>\n",
       "      <td>3447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>monde</td>\n",
       "      <td>3429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>depuis</td>\n",
       "      <td>3420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>ils</td>\n",
       "      <td>3339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>séries</td>\n",
       "      <td>3321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>entre</td>\n",
       "      <td>3312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>politique</td>\n",
       "      <td>3287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>français</td>\n",
       "      <td>3284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>netflixfr</td>\n",
       "      <td>3244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>2015</td>\n",
       "      <td>3242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>nouvelle</td>\n",
       "      <td>3193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>contre</td>\n",
       "      <td>3177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>the</td>\n",
       "      <td>3145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>premier</td>\n",
       "      <td>3144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>avant</td>\n",
       "      <td>3088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>truc</td>\n",
       "      <td>3051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>ines_assia</td>\n",
       "      <td>2902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>benoît</td>\n",
       "      <td>2881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>film</td>\n",
       "      <td>2872</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               words  count\n",
       "0          marseille  39257\n",
       "1              série  24919\n",
       "2            netflix  20464\n",
       "3               plus  12718\n",
       "4               2016  10807\n",
       "5   marseillenetflix  10587\n",
       "6          depardieu   8659\n",
       "7             france   8531\n",
       "8               bien   7588\n",
       "9                mai   7462\n",
       "10            gérard   6596\n",
       "11              fait   6457\n",
       "12              tout   5846\n",
       "13          première   5443\n",
       "14             comme   5439\n",
       "15               ans   5243\n",
       "16              deux   5018\n",
       "17             après   4933\n",
       "18             ville   4764\n",
       "19             serie   4670\n",
       "20            saison   4582\n",
       "21              être   4461\n",
       "22         française   4446\n",
       "23    marseilleserie   4446\n",
       "24           magimel   4315\n",
       "25              très   4115\n",
       "26             maire   3967\n",
       "27               tf1   3924\n",
       "28             faire   3882\n",
       "29             aussi   3818\n",
       "30              seul   3782\n",
       "31              sans   3624\n",
       "32          épisodes   3447\n",
       "33             monde   3429\n",
       "34            depuis   3420\n",
       "35               ils   3339\n",
       "36            séries   3321\n",
       "37             entre   3312\n",
       "38         politique   3287\n",
       "39          français   3284\n",
       "40         netflixfr   3244\n",
       "41              2015   3242\n",
       "42          nouvelle   3193\n",
       "43            contre   3177\n",
       "44               the   3145\n",
       "45           premier   3144\n",
       "46             avant   3088\n",
       "47              truc   3051\n",
       "48        ines_assia   2902\n",
       "49            benoît   2881\n",
       "50              film   2872"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sum up the counts of each vocabulary word\n",
    "dist = np.sum(train_data_features, axis=0)\n",
    "\n",
    "vocabdf = pd.DataFrame()\n",
    "vocabdf['words'] = vocab\n",
    "vocabdf['count'] = dist\n",
    "vocabdf = vocabdf.sort_values('count',ascending=False)\n",
    "vocabdf = vocabdf.reset_index(drop=True)\n",
    "# Saving the data\n",
    "vocabdf.to_pickle(pickle_data_path+series_name+'_vocab_bow'+'.pkl')\n",
    "vocabdf.loc[0:50,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating the TFIDF vectors...\n",
      "\n",
      "Nb of documents: 32394, Nb of features: 5000\n"
     ]
    }
   ],
   "source": [
    "print(\"Creating the TFIDF vectors...\\n\")\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Initialize the \"CountVectorizer\" object, which is scikit-learn's\n",
    "# bag of words tool.  \n",
    "vectorizer = TfidfVectorizer(analyzer = \"word\",   \\\n",
    "                             tokenizer = None,    \\\n",
    "                             preprocessor = None, \\\n",
    "                             stop_words = None,   \\\n",
    "                             max_features = 5000) \n",
    "\n",
    "# fit_transform() does two functions: First, it fits the model\n",
    "# and learns the vocabulary; second, it transforms our training data\n",
    "# into feature vectors. The input to fit_transform should be a list of \n",
    "# strings.\n",
    "train_data_features_tfidf = vectorizer.fit_transform(clean_train_reviews)\n",
    "\n",
    "# Numpy arrays are easy to work with, so convert the result to an \n",
    "# array\n",
    "train_data_features_tfidf = train_data_features_tfidf.toarray()\n",
    "vocab_tfidf = vectorizer.get_feature_names()\n",
    "print('Nb of documents: '+str(train_data_features_tfidf.shape[0])+', '+\n",
    "      'Nb of features: '+str(train_data_features_tfidf.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>marseille</td>\n",
       "      <td>2851.256342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>marseillenetflix</td>\n",
       "      <td>2115.469048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>série</td>\n",
       "      <td>2110.440963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>netflix</td>\n",
       "      <td>1853.886850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>serie</td>\n",
       "      <td>1734.384797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>bien</td>\n",
       "      <td>1407.051365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ines_assia</td>\n",
       "      <td>1377.566286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>truc</td>\n",
       "      <td>1376.855708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>seul</td>\n",
       "      <td>1331.194686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>marseilleserie</td>\n",
       "      <td>1205.915745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>depardieu</td>\n",
       "      <td>1114.503578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>netflixfr</td>\n",
       "      <td>917.744703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>mai</td>\n",
       "      <td>724.527446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>gérard</td>\n",
       "      <td>698.466377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>tf1</td>\n",
       "      <td>592.536970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>française</td>\n",
       "      <td>573.503331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>magimel</td>\n",
       "      <td>538.836859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>maire</td>\n",
       "      <td>499.830359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>plus</td>\n",
       "      <td>490.748824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>première</td>\n",
       "      <td>471.295640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>saison</td>\n",
       "      <td>442.531429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>racistes</td>\n",
       "      <td>414.492077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>fait</td>\n",
       "      <td>388.701063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>ville</td>\n",
       "      <td>376.232252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2016</td>\n",
       "      <td>364.343828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>maintenant</td>\n",
       "      <td>360.818783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>dès</td>\n",
       "      <td>358.610194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>rivalité</td>\n",
       "      <td>342.435407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>liberté</td>\n",
       "      <td>337.719722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>égalité</td>\n",
       "      <td>333.729974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>france</td>\n",
       "      <td>328.826635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>premier</td>\n",
       "      <td>326.044149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>tout</td>\n",
       "      <td>325.924109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>disponible</td>\n",
       "      <td>324.538094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>épisodes</td>\n",
       "      <td>320.189915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>benoît</td>\n",
       "      <td>308.905541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>comme</td>\n",
       "      <td>304.180442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>tournage</td>\n",
       "      <td>303.883790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>nouvelle</td>\n",
       "      <td>295.835076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>deux</td>\n",
       "      <td>265.668844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>teaser</td>\n",
       "      <td>263.290996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>via</td>\n",
       "      <td>256.017747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>avril</td>\n",
       "      <td>235.581916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>quand</td>\n",
       "      <td>234.825546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>très</td>\n",
       "      <td>226.380713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>faire</td>\n",
       "      <td>220.256139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>premiers</td>\n",
       "      <td>220.214195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>oitnb</td>\n",
       "      <td>219.579715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>après</td>\n",
       "      <td>219.361192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>séries</td>\n",
       "      <td>215.608543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>graceandfrankie</td>\n",
       "      <td>215.109340</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               words        count\n",
       "0          marseille  2851.256342\n",
       "1   marseillenetflix  2115.469048\n",
       "2              série  2110.440963\n",
       "3            netflix  1853.886850\n",
       "4              serie  1734.384797\n",
       "5               bien  1407.051365\n",
       "6         ines_assia  1377.566286\n",
       "7               truc  1376.855708\n",
       "8               seul  1331.194686\n",
       "9     marseilleserie  1205.915745\n",
       "10         depardieu  1114.503578\n",
       "11         netflixfr   917.744703\n",
       "12               mai   724.527446\n",
       "13            gérard   698.466377\n",
       "14               tf1   592.536970\n",
       "15         française   573.503331\n",
       "16           magimel   538.836859\n",
       "17             maire   499.830359\n",
       "18              plus   490.748824\n",
       "19          première   471.295640\n",
       "20            saison   442.531429\n",
       "21          racistes   414.492077\n",
       "22              fait   388.701063\n",
       "23             ville   376.232252\n",
       "24              2016   364.343828\n",
       "25        maintenant   360.818783\n",
       "26               dès   358.610194\n",
       "27          rivalité   342.435407\n",
       "28           liberté   337.719722\n",
       "29           égalité   333.729974\n",
       "30            france   328.826635\n",
       "31           premier   326.044149\n",
       "32              tout   325.924109\n",
       "33        disponible   324.538094\n",
       "34          épisodes   320.189915\n",
       "35            benoît   308.905541\n",
       "36             comme   304.180442\n",
       "37          tournage   303.883790\n",
       "38          nouvelle   295.835076\n",
       "39              deux   265.668844\n",
       "40            teaser   263.290996\n",
       "41               via   256.017747\n",
       "42             avril   235.581916\n",
       "43             quand   234.825546\n",
       "44              très   226.380713\n",
       "45             faire   220.256139\n",
       "46          premiers   220.214195\n",
       "47             oitnb   219.579715\n",
       "48             après   219.361192\n",
       "49            séries   215.608543\n",
       "50   graceandfrankie   215.109340"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sum up the counts of each vocabulary word\n",
    "dist_tfidf = np.sum(train_data_features_tfidf, axis=0)\n",
    "\n",
    "vocabdf = pd.DataFrame()\n",
    "vocabdf['words'] = vocab_tfidf\n",
    "vocabdf['count'] = dist_tfidf\n",
    "vocabdf = vocabdf.sort_values('count',ascending=False)\n",
    "vocabdf = vocabdf.reset_index(drop=True)\n",
    "# Saving the data\n",
    "vocabdf.to_pickle(pickle_data_path+series_name+'_vocab_tfidf'+'.pkl')\n",
    "vocabdf.loc[0:50,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ai',\n",
       " 'aie',\n",
       " 'aient',\n",
       " 'aies',\n",
       " 'ait',\n",
       " 'as',\n",
       " 'au',\n",
       " 'aura',\n",
       " 'aurai',\n",
       " 'auraient',\n",
       " 'aurais',\n",
       " 'aurait',\n",
       " 'auras',\n",
       " 'aurez',\n",
       " 'auriez',\n",
       " 'aurions',\n",
       " 'aurons',\n",
       " 'auront',\n",
       " 'aux',\n",
       " 'avaient',\n",
       " 'avais',\n",
       " 'avait',\n",
       " 'avec',\n",
       " 'avez',\n",
       " 'aviez',\n",
       " 'avions',\n",
       " 'avons',\n",
       " 'ayant',\n",
       " 'ayante',\n",
       " 'ayantes',\n",
       " 'ayants',\n",
       " 'ayez',\n",
       " 'ayons',\n",
       " 'c',\n",
       " 'ce',\n",
       " 'ces',\n",
       " 'cette',\n",
       " 'd',\n",
       " 'dans',\n",
       " 'de',\n",
       " 'des',\n",
       " 'du',\n",
       " 'elle',\n",
       " 'en',\n",
       " 'es',\n",
       " 'est',\n",
       " 'et',\n",
       " 'eu',\n",
       " 'eue',\n",
       " 'eues',\n",
       " 'eurent',\n",
       " 'eus',\n",
       " 'eusse',\n",
       " 'eussent',\n",
       " 'eusses',\n",
       " 'eussiez',\n",
       " 'eussions',\n",
       " 'eut',\n",
       " 'eux',\n",
       " 'eûmes',\n",
       " 'eût',\n",
       " 'eûtes',\n",
       " 'furent',\n",
       " 'fus',\n",
       " 'fusse',\n",
       " 'fussent',\n",
       " 'fusses',\n",
       " 'fussiez',\n",
       " 'fussions',\n",
       " 'fut',\n",
       " 'fûmes',\n",
       " 'fût',\n",
       " 'fûtes',\n",
       " 'http',\n",
       " 'https',\n",
       " 'il',\n",
       " 'j',\n",
       " 'je',\n",
       " 'l',\n",
       " 'la',\n",
       " 'le',\n",
       " 'les',\n",
       " 'leur',\n",
       " 'lui',\n",
       " 'm',\n",
       " 'ma',\n",
       " 'mais',\n",
       " 'me',\n",
       " 'mes',\n",
       " 'moi',\n",
       " 'mon',\n",
       " 'même',\n",
       " 'n',\n",
       " 'ne',\n",
       " 'nos',\n",
       " 'notre',\n",
       " 'nous',\n",
       " 'on',\n",
       " 'ont',\n",
       " 'ou',\n",
       " 'par',\n",
       " 'pas',\n",
       " 'pour',\n",
       " 'qu',\n",
       " 'que',\n",
       " 'qui',\n",
       " 's',\n",
       " 'sa',\n",
       " 'se',\n",
       " 'sera',\n",
       " 'serai',\n",
       " 'seraient',\n",
       " 'serais',\n",
       " 'serait',\n",
       " 'seras',\n",
       " 'serez',\n",
       " 'seriez',\n",
       " 'serions',\n",
       " 'serons',\n",
       " 'seront',\n",
       " 'ses',\n",
       " 'soient',\n",
       " 'sois',\n",
       " 'soit',\n",
       " 'sommes',\n",
       " 'son',\n",
       " 'sont',\n",
       " 'soyez',\n",
       " 'soyons',\n",
       " 'suis',\n",
       " 'sur',\n",
       " 't',\n",
       " 'ta',\n",
       " 'te',\n",
       " 'tes',\n",
       " 'toi',\n",
       " 'ton',\n",
       " 'tu',\n",
       " 'un',\n",
       " 'une',\n",
       " 'vos',\n",
       " 'votre',\n",
       " 'vous',\n",
       " 'y',\n",
       " 'à',\n",
       " 'étaient',\n",
       " 'étais',\n",
       " 'était',\n",
       " 'étant',\n",
       " 'étante',\n",
       " 'étantes',\n",
       " 'étants',\n",
       " 'étiez',\n",
       " 'étions',\n",
       " 'été',\n",
       " 'étée',\n",
       " 'étées',\n",
       " 'étés',\n",
       " 'êtes'}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "french_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
